{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e96518f-e1e5-41d0-8cd8-9d44e027789c",
   "metadata": {},
   "source": [
    "# VAE LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "928d86d9-a79a-4544-a056-633b1b791888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55c8e28b-4383-44d8-af49-12f6eab048cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data=pd.read_csv(r\"C:\\Users\\dwill\\OneDrive\\Desktop\\INTERN WORKS\\wind_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ebc1ce4-350b-44b4-a160-97f58cda5108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TurbID  Day Tmstamp  Wspd  Wdir   Etmp   Itmp   Ndir  Pab1  Pab2  Pab3  \\\n",
      "0       1    1   00:10  6.17 -3.99  30.73  41.80  25.92   1.0   1.0   1.0   \n",
      "1       1    1   00:20  6.27 -2.18  30.60  41.63  20.91   1.0   1.0   1.0   \n",
      "2       1    1   00:30  6.42 -0.73  30.52  41.52  20.91   1.0   1.0   1.0   \n",
      "3       1    1   00:40  6.25  0.89  30.49  41.38  20.91   1.0   1.0   1.0   \n",
      "4       1    1   00:50  6.10 -1.03  30.47  41.22  20.91   1.0   1.0   1.0   \n",
      "\n",
      "   Prtv    Patv  \n",
      "0 -0.25  494.66  \n",
      "1 -0.24  509.76  \n",
      "2 -0.26  542.53  \n",
      "3 -0.23  509.36  \n",
      "4 -0.27  482.21  \n",
      "(10000, 13)\n"
     ]
    }
   ],
   "source": [
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2d9931c-e937-47f1-b416-299db6754898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TurbID     0\n",
       "Day        0\n",
       "Tmstamp    0\n",
       "Wspd       0\n",
       "Wdir       0\n",
       "Etmp       0\n",
       "Itmp       0\n",
       "Ndir       0\n",
       "Pab1       0\n",
       "Pab2       0\n",
       "Pab3       0\n",
       "Prtv       0\n",
       "Patv       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abbb0aef-5bc1-416d-9acf-1640c0015d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TurbID     0\n",
       "Day        0\n",
       "Tmstamp    0\n",
       "Wspd       0\n",
       "Wdir       0\n",
       "Etmp       0\n",
       "Itmp       0\n",
       "Ndir       0\n",
       "Pab1       0\n",
       "Pab2       0\n",
       "Pab3       0\n",
       "Prtv       0\n",
       "Patv       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data.dropna()\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1015875-8bbf-407d-8c11-7f8f3ac78829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 13)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42dda361-881f-4744-8217-6f8f557dbabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   TurbID  10000 non-null  int64  \n",
      " 1   Day     10000 non-null  int64  \n",
      " 2   Wspd    10000 non-null  float64\n",
      " 3   Wdir    10000 non-null  float64\n",
      " 4   Etmp    10000 non-null  float64\n",
      " 5   Itmp    10000 non-null  float64\n",
      " 6   Ndir    10000 non-null  float64\n",
      " 7   Pab1    10000 non-null  float64\n",
      " 8   Pab2    10000 non-null  float64\n",
      " 9   Pab3    10000 non-null  float64\n",
      " 10  Prtv    10000 non-null  float64\n",
      " 11  Patv    10000 non-null  float64\n",
      " 12  hour    10000 non-null  int64  \n",
      " 13  minute  10000 non-null  int64  \n",
      "dtypes: float64(10), int64(4)\n",
      "memory usage: 1.1 MB\n"
     ]
    }
   ],
   "source": [
    "data[['hour', 'minute']] = data['Tmstamp'].str.split(':', expand=True)\n",
    "data['hour'] = pd.to_numeric(data['hour'])\n",
    "data['minute'] = pd.to_numeric(data['minute'])\n",
    "data.drop(columns=['Tmstamp'], inplace=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74dcb75c-9352-4411-8c23-b9476eee6725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      TurbID  Day  hour  minute  Wspd  Wdir   Etmp   Itmp    Ndir  Pab1  Pab2  \\\n",
      "0          1    1     0      10  6.17 -3.99  30.73  41.80   25.92  1.00  1.00   \n",
      "1          1    1     0      20  6.27 -2.18  30.60  41.63   20.91  1.00  1.00   \n",
      "2          1    1     0      30  6.42 -0.73  30.52  41.52   20.91  1.00  1.00   \n",
      "3          1    1     0      40  6.25  0.89  30.49  41.38   20.91  1.00  1.00   \n",
      "4          1    1     0      50  6.10 -1.03  30.47  41.22   20.91  1.00  1.00   \n",
      "...      ...  ...   ...     ...   ...   ...    ...    ...     ...   ...   ...   \n",
      "9995       1   72     2      10  6.72  4.66  32.53  43.40  404.58  1.00  1.00   \n",
      "9996       1   72     2      20  4.25 -2.10  32.33  42.71  413.38  0.99  0.99   \n",
      "9997       1   72     2      30  3.50  3.53  32.26  42.46  411.41  0.99  0.99   \n",
      "9998       1   72     2      40  5.39  4.66  32.11  42.31  425.11  0.99  0.99   \n",
      "9999       1   72     2      50  6.23 -0.43  31.93  42.25  444.62  1.00  1.00   \n",
      "\n",
      "      Pab3  Prtv    Patv  \n",
      "0     1.00 -0.25  494.66  \n",
      "1     1.00 -0.24  509.76  \n",
      "2     1.00 -0.26  542.53  \n",
      "3     1.00 -0.23  509.36  \n",
      "4     1.00 -0.27  482.21  \n",
      "...    ...   ...     ...  \n",
      "9995  1.00 -0.22  542.20  \n",
      "9996  0.99 -0.21  195.72  \n",
      "9997  0.99 -0.23  130.72  \n",
      "9998  0.99 -0.25  313.23  \n",
      "9999  1.00 -0.24  441.01  \n",
      "\n",
      "[10000 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "desired_order = [\n",
    "    'TurbID', 'Day', 'hour', 'minute','Wspd', 'Wdir', 'Etmp', 'Itmp', 'Ndir',\n",
    "    'Pab1', 'Pab2', 'Pab3', 'Prtv', 'Patv']\n",
    "data = data.reindex(columns=desired_order)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5b7c12c-5c42-4028-9bb5-29a2ea94ca31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "      TurbID  Day  hour  minute  Wspd  Wdir   Etmp   Itmp    Ndir  Pab1  Pab2  \\\n",
      "0          1    1     0      10  6.17 -3.99  30.73  41.80   25.92  1.00  1.00   \n",
      "1          1    1     0      20  6.27 -2.18  30.60  41.63   20.91  1.00  1.00   \n",
      "2          1    1     0      30  6.42 -0.73  30.52  41.52   20.91  1.00  1.00   \n",
      "3          1    1     0      40  6.25  0.89  30.49  41.38   20.91  1.00  1.00   \n",
      "4          1    1     0      50  6.10 -1.03  30.47  41.22   20.91  1.00  1.00   \n",
      "...      ...  ...   ...     ...   ...   ...    ...    ...     ...   ...   ...   \n",
      "9995       1   72     2      10  6.72  4.66  32.53  43.40  404.58  1.00  1.00   \n",
      "9996       1   72     2      20  4.25 -2.10  32.33  42.71  413.38  0.99  0.99   \n",
      "9997       1   72     2      30  3.50  3.53  32.26  42.46  411.41  0.99  0.99   \n",
      "9998       1   72     2      40  5.39  4.66  32.11  42.31  425.11  0.99  0.99   \n",
      "9999       1   72     2      50  6.23 -0.43  31.93  42.25  444.62  1.00  1.00   \n",
      "\n",
      "      Pab3  Prtv    Patv  \n",
      "0     1.00 -0.25  494.66  \n",
      "1     1.00 -0.24  509.76  \n",
      "2     1.00 -0.26  542.53  \n",
      "3     1.00 -0.23  509.36  \n",
      "4     1.00 -0.27  482.21  \n",
      "...    ...   ...     ...  \n",
      "9995  1.00 -0.22  542.20  \n",
      "9996  0.99 -0.21  195.72  \n",
      "9997  0.99 -0.23  130.72  \n",
      "9998  0.99 -0.25  313.23  \n",
      "9999  1.00 -0.24  441.01  \n",
      "\n",
      "[10000 rows x 14 columns]\n",
      "\n",
      "DataFrame after dropping outliers:\n",
      "      TurbID  Day  hour  minute  Wspd  Wdir   Etmp   Itmp    Ndir  Pab1  Pab2  \\\n",
      "0          1    1     0      10  6.17 -3.99  30.73  41.80   25.92  1.00  1.00   \n",
      "1          1    1     0      20  6.27 -2.18  30.60  41.63   20.91  1.00  1.00   \n",
      "2          1    1     0      30  6.42 -0.73  30.52  41.52   20.91  1.00  1.00   \n",
      "3          1    1     0      40  6.25  0.89  30.49  41.38   20.91  1.00  1.00   \n",
      "4          1    1     0      50  6.10 -1.03  30.47  41.22   20.91  1.00  1.00   \n",
      "...      ...  ...   ...     ...   ...   ...    ...    ...     ...   ...   ...   \n",
      "9995       1   72     2      10  6.72  4.66  32.53  43.40  404.58  1.00  1.00   \n",
      "9996       1   72     2      20  4.25 -2.10  32.33  42.71  413.38  0.99  0.99   \n",
      "9997       1   72     2      30  3.50  3.53  32.26  42.46  411.41  0.99  0.99   \n",
      "9998       1   72     2      40  5.39  4.66  32.11  42.31  425.11  0.99  0.99   \n",
      "9999       1   72     2      50  6.23 -0.43  31.93  42.25  444.62  1.00  1.00   \n",
      "\n",
      "      Pab3  Prtv    Patv  \n",
      "0     1.00 -0.25  494.66  \n",
      "1     1.00 -0.24  509.76  \n",
      "2     1.00 -0.26  542.53  \n",
      "3     1.00 -0.23  509.36  \n",
      "4     1.00 -0.27  482.21  \n",
      "...    ...   ...     ...  \n",
      "9995  1.00 -0.22  542.20  \n",
      "9996  0.99 -0.21  195.72  \n",
      "9997  0.99 -0.23  130.72  \n",
      "9998  0.99 -0.25  313.23  \n",
      "9999  1.00 -0.24  441.01  \n",
      "\n",
      "[6951 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def drop_outliers_iqr(dataframe):\n",
    "    q1 = dataframe.quantile(0.25)\n",
    "    q3 = dataframe.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    #1.5 is threshold\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    outliers = ((dataframe < lower_bound) | (dataframe > upper_bound)).any(axis=1)\n",
    "    cleaned_dataframe = dataframe[~outliers]\n",
    "    return cleaned_dataframe\n",
    "cleaned_df = drop_outliers_iqr(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(data)\n",
    "print(\"\\nDataFrame after dropping outliers:\")\n",
    "print(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "122ba7ff-8fc9-49d2-a33c-98ca6f8901a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      TurbID  Day      hour  minute      Wspd      Wdir      Etmp      Itmp  \\\n",
      "0        0.0  0.0  0.000000     0.2  0.364055  0.310772  0.611111  0.625951   \n",
      "1        0.0  0.0  0.000000     0.4  0.370639  0.397045  0.606421  0.619790   \n",
      "2        0.0  0.0  0.000000     0.6  0.380513  0.466158  0.603535  0.615803   \n",
      "3        0.0  0.0  0.000000     0.8  0.369322  0.543375  0.602453  0.610729   \n",
      "4        0.0  0.0  0.000000     1.0  0.359447  0.451859  0.601732  0.604929   \n",
      "...      ...  ...       ...     ...       ...       ...       ...       ...   \n",
      "6946     0.0  1.0  0.086957     0.2  0.400263  0.723070  0.676046  0.683943   \n",
      "6947     0.0  1.0  0.086957     0.4  0.237656  0.400858  0.668831  0.658934   \n",
      "6948     0.0  1.0  0.086957     0.6  0.188282  0.669209  0.666306  0.649873   \n",
      "6949     0.0  1.0  0.086957     0.8  0.312706  0.723070  0.660895  0.644436   \n",
      "6950     0.0  1.0  0.086957     1.0  0.368005  0.480458  0.654401  0.642262   \n",
      "\n",
      "          Ndir      Pab1      Pab2      Pab3      Prtv      Patv  \n",
      "0     0.271912  0.010989  0.015385  0.010989  0.500000  0.320070  \n",
      "1     0.266146  0.010989  0.015385  0.010989  0.541667  0.329787  \n",
      "2     0.266146  0.010989  0.015385  0.010989  0.458333  0.350877  \n",
      "3     0.266146  0.010989  0.015385  0.010989  0.583333  0.329530  \n",
      "4     0.266146  0.010989  0.015385  0.010989  0.416667  0.312057  \n",
      "...        ...       ...       ...       ...       ...       ...  \n",
      "6946  0.707750  0.010989  0.015385  0.010989  0.625000  0.350664  \n",
      "6947  0.717878  0.000000  0.000000  0.000000  0.666667  0.127683  \n",
      "6948  0.715611  0.000000  0.000000  0.000000  0.583333  0.085851  \n",
      "6949  0.731380  0.000000  0.000000  0.000000  0.500000  0.203308  \n",
      "6950  0.753836  0.010989  0.015385  0.010989  0.541667  0.285542  \n",
      "\n",
      "[6951 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "cleaned_df_normalized = scaler.fit_transform(cleaned_df)\n",
    "cleaned_df_normalized_df = pd.DataFrame(cleaned_df_normalized, columns=cleaned_df.columns)\n",
    "print(cleaned_df_normalized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b829c29-479c-49da-bd77-d0dc9a861ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=cleaned_df_normalized_df.drop(columns=['Patv'])\n",
    "y=cleaned_df_normalized_df[\"Patv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10e78e34-252e-4a1a-984f-8ac91336aee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Wspd      Etmp      Itmp      Pab1      Pab2      Pab3\n",
      "0     0.364055  0.611111  0.625951  0.010989  0.015385  0.010989\n",
      "1     0.370639  0.606421  0.619790  0.010989  0.015385  0.010989\n",
      "2     0.380513  0.603535  0.615803  0.010989  0.015385  0.010989\n",
      "3     0.369322  0.602453  0.610729  0.010989  0.015385  0.010989\n",
      "4     0.359447  0.601732  0.604929  0.010989  0.015385  0.010989\n",
      "...        ...       ...       ...       ...       ...       ...\n",
      "6946  0.400263  0.676046  0.683943  0.010989  0.015385  0.010989\n",
      "6947  0.237656  0.668831  0.658934  0.000000  0.000000  0.000000\n",
      "6948  0.188282  0.666306  0.649873  0.000000  0.000000  0.000000\n",
      "6949  0.312706  0.660895  0.644436  0.000000  0.000000  0.000000\n",
      "6950  0.368005  0.654401  0.642262  0.010989  0.015385  0.010989\n",
      "\n",
      "[6951 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# wrapper method\n",
    "# Recursive Feature Elimination (RFE)\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "rfe = RFE(model, n_features_to_select=6) \n",
    "rfe = rfe.fit(x, y)\n",
    "# Get selected features\n",
    "selected_features = x.columns[rfe.support_]\n",
    "df_reduced = x[selected_features]\n",
    "print(df_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a96fc4b-4bf3-45a7-b46b-0019a13a0f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6255, 6), (696,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(df_reduced,y,test_size=0.1,random_state=42)\n",
    "x_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fdf7497-c976-4c52-bc29-0c62d85ca578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d2f2807-91cf-4e93-9827-2418f6870160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall tensorflow -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79febf9e-03fc-4022-ac67-680b405ae7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "517b6079-f88a-42fa-a29a-a4ce04927bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dwill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\dwill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:From C:\\Users\\dwill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "98/98 [==============================] - 15s 42ms/step - loss: 0.0913 - val_loss: 0.0803\n",
      "Epoch 2/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0765 - val_loss: 0.0772\n",
      "Epoch 3/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0738 - val_loss: 0.0738\n",
      "Epoch 4/200\n",
      "98/98 [==============================] - 3s 28ms/step - loss: 0.0698 - val_loss: 0.0710\n",
      "Epoch 5/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0722 - val_loss: 0.0729\n",
      "Epoch 6/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0723 - val_loss: 0.0721\n",
      "Epoch 7/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0669 - val_loss: 0.0759\n",
      "Epoch 8/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0684 - val_loss: 0.0716\n",
      "Epoch 9/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0690 - val_loss: 0.0704\n",
      "Epoch 10/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0672 - val_loss: 0.0797\n",
      "Epoch 11/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0680 - val_loss: 0.0714\n",
      "Epoch 12/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0699 - val_loss: 0.0727\n",
      "Epoch 13/200\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 0.0653 - val_loss: 0.0703\n",
      "Epoch 14/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0683 - val_loss: 0.0702\n",
      "Epoch 15/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0666 - val_loss: 0.0746\n",
      "Epoch 16/200\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 0.0663 - val_loss: 0.0707\n",
      "Epoch 17/200\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 0.0679 - val_loss: 0.0683\n",
      "Epoch 18/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0660 - val_loss: 0.0722\n",
      "Epoch 19/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0657 - val_loss: 0.0698\n",
      "Epoch 20/200\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 0.0683 - val_loss: 0.0705\n",
      "Epoch 21/200\n",
      "98/98 [==============================] - 3s 28ms/step - loss: 0.0678 - val_loss: 0.0695\n",
      "Epoch 22/200\n",
      "98/98 [==============================] - 3s 28ms/step - loss: 0.0681 - val_loss: 0.0708\n",
      "Epoch 23/200\n",
      "98/98 [==============================] - 3s 28ms/step - loss: 0.0658 - val_loss: 0.0805\n",
      "Epoch 24/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0699 - val_loss: 0.0703\n",
      "Epoch 25/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0661 - val_loss: 0.0722\n",
      "Epoch 26/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0702 - val_loss: 0.0702\n",
      "Epoch 27/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0680 - val_loss: 0.0698\n",
      "Epoch 28/200\n",
      "98/98 [==============================] - 3s 28ms/step - loss: 0.0686 - val_loss: 0.0692\n",
      "Epoch 29/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0659 - val_loss: 0.0742\n",
      "Epoch 30/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0653 - val_loss: 0.0711\n",
      "Epoch 31/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0685 - val_loss: 0.0703\n",
      "Epoch 32/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0636 - val_loss: 0.0695\n",
      "Epoch 33/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0672 - val_loss: 0.0695\n",
      "Epoch 34/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0666 - val_loss: 0.0694\n",
      "Epoch 35/200\n",
      "98/98 [==============================] - 3s 28ms/step - loss: 0.0677 - val_loss: 0.0702\n",
      "Epoch 36/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0661 - val_loss: 0.0701\n",
      "Epoch 37/200\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 0.0672 - val_loss: 0.0690\n",
      "Epoch 38/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0660 - val_loss: 0.0691\n",
      "Epoch 39/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0647 - val_loss: 0.0699\n",
      "Epoch 40/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0684 - val_loss: 0.0692\n",
      "Epoch 41/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0657 - val_loss: 0.0729\n",
      "Epoch 42/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0653 - val_loss: 0.0730\n",
      "Epoch 43/200\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 0.0687 - val_loss: 0.0693\n",
      "Epoch 44/200\n",
      "98/98 [==============================] - 2s 18ms/step - loss: 0.0680 - val_loss: 0.0692\n",
      "Epoch 45/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0641 - val_loss: 0.0690\n",
      "Epoch 46/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0670 - val_loss: 0.0713\n",
      "Epoch 47/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0686 - val_loss: 0.0710\n",
      "Epoch 48/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0674 - val_loss: 0.0702\n",
      "Epoch 49/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0675 - val_loss: 0.0723\n",
      "Epoch 50/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0676 - val_loss: 0.0699\n",
      "Epoch 51/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0676 - val_loss: 0.0715\n",
      "Epoch 52/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0656 - val_loss: 0.0734\n",
      "Epoch 53/200\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 0.0656 - val_loss: 0.0695\n",
      "Epoch 54/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0676 - val_loss: 0.0694\n",
      "Epoch 55/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0662 - val_loss: 0.0716\n",
      "Epoch 56/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0664 - val_loss: 0.0691\n",
      "Epoch 57/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0673 - val_loss: 0.0712\n",
      "Epoch 58/200\n",
      "98/98 [==============================] - 3s 33ms/step - loss: 0.0656 - val_loss: 0.0709\n",
      "Epoch 59/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0673 - val_loss: 0.0697\n",
      "Epoch 60/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0654 - val_loss: 0.0701\n",
      "Epoch 61/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0662 - val_loss: 0.0709\n",
      "Epoch 62/200\n",
      "98/98 [==============================] - 3s 28ms/step - loss: 0.0652 - val_loss: 0.0738\n",
      "Epoch 63/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0683 - val_loss: 0.0692\n",
      "Epoch 64/200\n",
      "98/98 [==============================] - 3s 28ms/step - loss: 0.0678 - val_loss: 0.0694\n",
      "Epoch 65/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0677 - val_loss: 0.0693\n",
      "Epoch 66/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0667 - val_loss: 0.0732\n",
      "Epoch 67/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0659 - val_loss: 0.0735\n",
      "Epoch 68/200\n",
      "98/98 [==============================] - 3s 28ms/step - loss: 0.0681 - val_loss: 0.0704\n",
      "Epoch 69/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0659 - val_loss: 0.0689\n",
      "Epoch 70/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0671 - val_loss: 0.0695\n",
      "Epoch 71/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0680 - val_loss: 0.0711\n",
      "Epoch 72/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0671 - val_loss: 0.0693\n",
      "Epoch 73/200\n",
      "98/98 [==============================] - 3s 28ms/step - loss: 0.0649 - val_loss: 0.0716\n",
      "Epoch 74/200\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 0.0648 - val_loss: 0.0704\n",
      "Epoch 75/200\n",
      "98/98 [==============================] - 2s 16ms/step - loss: 0.0666 - val_loss: 0.0697\n",
      "Epoch 76/200\n",
      "98/98 [==============================] - 3s 27ms/step - loss: 0.0661 - val_loss: 0.0694\n",
      "Epoch 77/200\n",
      "98/98 [==============================] - 3s 28ms/step - loss: 0.0632 - val_loss: 0.0717\n",
      "Epoch 78/200\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 0.0632 - val_loss: 0.0690\n",
      "Epoch 79/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0678 - val_loss: 0.0708\n",
      "Epoch 80/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0655 - val_loss: 0.0706\n",
      "Epoch 81/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0679 - val_loss: 0.0701\n",
      "Epoch 82/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0666 - val_loss: 0.0700\n",
      "Epoch 83/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0656 - val_loss: 0.0727\n",
      "Epoch 84/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0691 - val_loss: 0.0691\n",
      "Epoch 85/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0643 - val_loss: 0.0695\n",
      "Epoch 86/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0672 - val_loss: 0.0712\n",
      "Epoch 87/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0665 - val_loss: 0.0687\n",
      "Epoch 88/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0656 - val_loss: 0.0696\n",
      "Epoch 89/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0665 - val_loss: 0.0688\n",
      "Epoch 90/200\n",
      "98/98 [==============================] - 2s 16ms/step - loss: 0.0674 - val_loss: 0.0686\n",
      "Epoch 91/200\n",
      "98/98 [==============================] - 3s 27ms/step - loss: 0.0650 - val_loss: 0.0692\n",
      "Epoch 92/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0683 - val_loss: 0.0692\n",
      "Epoch 93/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0651 - val_loss: 0.0718\n",
      "Epoch 94/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0660 - val_loss: 0.0693\n",
      "Epoch 95/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0676 - val_loss: 0.0703\n",
      "Epoch 96/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0676 - val_loss: 0.0694\n",
      "Epoch 97/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0661 - val_loss: 0.0691\n",
      "Epoch 98/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0672 - val_loss: 0.0689\n",
      "Epoch 99/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0663 - val_loss: 0.0730\n",
      "Epoch 100/200\n",
      "98/98 [==============================] - 3s 28ms/step - loss: 0.0683 - val_loss: 0.0698\n",
      "Epoch 101/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0667 - val_loss: 0.0700\n",
      "Epoch 102/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0677 - val_loss: 0.0692\n",
      "Epoch 103/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0661 - val_loss: 0.0695\n",
      "Epoch 104/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0667 - val_loss: 0.0709\n",
      "Epoch 105/200\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 0.0682 - val_loss: 0.0702\n",
      "Epoch 106/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0652 - val_loss: 0.0708\n",
      "Epoch 107/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0662 - val_loss: 0.0698\n",
      "Epoch 108/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0663 - val_loss: 0.0717\n",
      "Epoch 109/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0676 - val_loss: 0.0691\n",
      "Epoch 110/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0655 - val_loss: 0.0683\n",
      "Epoch 111/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0650 - val_loss: 0.0696\n",
      "Epoch 112/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0634 - val_loss: 0.0693\n",
      "Epoch 113/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0675 - val_loss: 0.0706\n",
      "Epoch 114/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0679 - val_loss: 0.0691\n",
      "Epoch 115/200\n",
      "98/98 [==============================] - 2s 16ms/step - loss: 0.0665 - val_loss: 0.0700\n",
      "Epoch 116/200\n",
      "98/98 [==============================] - 3s 28ms/step - loss: 0.0667 - val_loss: 0.0693\n",
      "Epoch 117/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0691 - val_loss: 0.0724\n",
      "Epoch 118/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0702 - val_loss: 0.0718\n",
      "Epoch 119/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0690 - val_loss: 0.0701\n",
      "Epoch 120/200\n",
      "98/98 [==============================] - 2s 16ms/step - loss: 0.0646 - val_loss: 0.0691\n",
      "Epoch 121/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0650 - val_loss: 0.0702\n",
      "Epoch 122/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0654 - val_loss: 0.0685\n",
      "Epoch 123/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0649 - val_loss: 0.0687\n",
      "Epoch 124/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0687 - val_loss: 0.0699\n",
      "Epoch 125/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0666 - val_loss: 0.0700\n",
      "Epoch 126/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0676 - val_loss: 0.0686\n",
      "Epoch 127/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0655 - val_loss: 0.0699\n",
      "Epoch 128/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0668 - val_loss: 0.0706\n",
      "Epoch 129/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0665 - val_loss: 0.0687\n",
      "Epoch 130/200\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 0.0644 - val_loss: 0.0711\n",
      "Epoch 131/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0676 - val_loss: 0.0693\n",
      "Epoch 132/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0656 - val_loss: 0.0700\n",
      "Epoch 133/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0660 - val_loss: 0.0704\n",
      "Epoch 134/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0663 - val_loss: 0.0707\n",
      "Epoch 135/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0669 - val_loss: 0.0691\n",
      "Epoch 136/200\n",
      "98/98 [==============================] - 2s 16ms/step - loss: 0.0637 - val_loss: 0.0735\n",
      "Epoch 137/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0651 - val_loss: 0.0705\n",
      "Epoch 138/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0649 - val_loss: 0.0695\n",
      "Epoch 139/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0662 - val_loss: 0.0691\n",
      "Epoch 140/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0647 - val_loss: 0.0687\n",
      "Epoch 141/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0652 - val_loss: 0.0680\n",
      "Epoch 142/200\n",
      "98/98 [==============================] - 2s 21ms/step - loss: 0.0658 - val_loss: 0.0732\n",
      "Epoch 143/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0634 - val_loss: 0.0702\n",
      "Epoch 144/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0662 - val_loss: 0.0685\n",
      "Epoch 145/200\n",
      "98/98 [==============================] - 2s 16ms/step - loss: 0.0673 - val_loss: 0.0686\n",
      "Epoch 146/200\n",
      "98/98 [==============================] - 3s 27ms/step - loss: 0.0665 - val_loss: 0.0691\n",
      "Epoch 147/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0668 - val_loss: 0.0703\n",
      "Epoch 148/200\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 0.0676 - val_loss: 0.0691\n",
      "Epoch 149/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0646 - val_loss: 0.0691\n",
      "Epoch 150/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0673 - val_loss: 0.0695\n",
      "Epoch 151/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0639 - val_loss: 0.0687\n",
      "Epoch 152/200\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 0.0658 - val_loss: 0.0703\n",
      "Epoch 153/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0675 - val_loss: 0.0708\n",
      "Epoch 154/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0675 - val_loss: 0.0714\n",
      "Epoch 155/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0649 - val_loss: 0.0732\n",
      "Epoch 156/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0661 - val_loss: 0.0697\n",
      "Epoch 157/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0672 - val_loss: 0.0692\n",
      "Epoch 158/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0681 - val_loss: 0.0690\n",
      "Epoch 159/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0660 - val_loss: 0.0703\n",
      "Epoch 160/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0637 - val_loss: 0.0704\n",
      "Epoch 161/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0683 - val_loss: 0.0690\n",
      "Epoch 162/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0675 - val_loss: 0.0696\n",
      "Epoch 163/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0658 - val_loss: 0.0692\n",
      "Epoch 164/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0646 - val_loss: 0.0751\n",
      "Epoch 165/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0655 - val_loss: 0.0714\n",
      "Epoch 166/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0657 - val_loss: 0.0690\n",
      "Epoch 167/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0666 - val_loss: 0.0709\n",
      "Epoch 168/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0630 - val_loss: 0.0699\n",
      "Epoch 169/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0670 - val_loss: 0.0694\n",
      "Epoch 170/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0654 - val_loss: 0.0702\n",
      "Epoch 171/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0651 - val_loss: 0.0688\n",
      "Epoch 172/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0639 - val_loss: 0.0691\n",
      "Epoch 173/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0637 - val_loss: 0.0689\n",
      "Epoch 174/200\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 0.0640 - val_loss: 0.0691\n",
      "Epoch 175/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0645 - val_loss: 0.0702\n",
      "Epoch 176/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0660 - val_loss: 0.0683\n",
      "Epoch 177/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0654 - val_loss: 0.0703\n",
      "Epoch 178/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0659 - val_loss: 0.0687\n",
      "Epoch 179/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0667 - val_loss: 0.0696\n",
      "Epoch 180/200\n",
      "98/98 [==============================] - 2s 16ms/step - loss: 0.0637 - val_loss: 0.0697\n",
      "Epoch 181/200\n",
      "98/98 [==============================] - 3s 28ms/step - loss: 0.0651 - val_loss: 0.0692\n",
      "Epoch 182/200\n",
      "98/98 [==============================] - 3s 32ms/step - loss: 0.0634 - val_loss: 0.0708\n",
      "Epoch 183/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0661 - val_loss: 0.0698\n",
      "Epoch 184/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0656 - val_loss: 0.0697\n",
      "Epoch 185/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0658 - val_loss: 0.0681\n",
      "Epoch 186/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0650 - val_loss: 0.0701\n",
      "Epoch 187/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0679 - val_loss: 0.0704\n",
      "Epoch 188/200\n",
      "98/98 [==============================] - 3s 28ms/step - loss: 0.0655 - val_loss: 0.0688\n",
      "Epoch 189/200\n",
      "98/98 [==============================] - 3s 31ms/step - loss: 0.0676 - val_loss: 0.0790\n",
      "Epoch 190/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0658 - val_loss: 0.0695\n",
      "Epoch 191/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0661 - val_loss: 0.0727\n",
      "Epoch 192/200\n",
      "98/98 [==============================] - 3s 28ms/step - loss: 0.0645 - val_loss: 0.0704\n",
      "Epoch 193/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0672 - val_loss: 0.0712\n",
      "Epoch 194/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0662 - val_loss: 0.0700\n",
      "Epoch 195/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0667 - val_loss: 0.0695\n",
      "Epoch 196/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0672 - val_loss: 0.0703\n",
      "Epoch 197/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0654 - val_loss: 0.0714\n",
      "Epoch 198/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0655 - val_loss: 0.0693\n",
      "Epoch 199/200\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.0654 - val_loss: 0.0688\n",
      "Epoch 200/200\n",
      "98/98 [==============================] - 3s 29ms/step - loss: 0.0627 - val_loss: 0.0719\n",
      "196/196 [==============================] - 2s 8ms/step\n",
      "196/196 [==============================] - 2s 10ms/step\n",
      "Sequence Length: 6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models \n",
    "import numpy as np\n",
    "latent_dim = 6\n",
    "input_shape = 6\n",
    "conv_input_shape = (input_shape, 1)\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = layers.Input(shape=conv_input_shape)\n",
    "x = layers.Conv1D(128, kernel_size=3, activation='elu', padding='same')(encoder_inputs)\n",
    "x = layers.Conv1D(64, kernel_size=3, activation='elu', padding='same')(x)\n",
    "x = layers.Conv1D(32, kernel_size=3, activation='elu', padding='same')(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256, activation=tf.nn.leaky_relu)(x)  # Leaky ReLU activation\n",
    "x = layers.Dense(128, activation=tf.nn.leaky_relu)(x)  # Leaky ReLU activation\n",
    "x = layers.Dense(64, activation=tf.nn.leaky_relu)(x)  # Leaky ReLU activation\n",
    "z_mean = layers.Dense(latent_dim)(x)\n",
    "z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "# Sampler\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = layers.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(64, activation=tf.nn.leaky_relu)(decoder_inputs)  # Leaky ReLU activation\n",
    "x = layers.Dense(128, activation=tf.nn.leaky_relu)(x)  # Leaky ReLU activation\n",
    "x = layers.Dense(256, activation=tf.nn.leaky_relu)(x)  # Leaky ReLU activation\n",
    "x = layers.Dense(input_shape * 32, activation=tf.nn.leaky_relu)(x)  # To match the flattened Conv1D output\n",
    "x = layers.Reshape((input_shape, 32))(x)\n",
    "x = layers.Conv1D(32, kernel_size=3, activation='elu', padding='same')(x)\n",
    "x = layers.Conv1D(64, kernel_size=3, activation='elu', padding='same')(x)\n",
    "x = layers.Conv1D(128, kernel_size=3, activation='elu', padding='same')(x)\n",
    "decoder_outputs = layers.Conv1D(1, kernel_size=3, activation='sigmoid', padding='same')(x)\n",
    "decoder_outputs = layers.Flatten()(decoder_outputs)\n",
    "\n",
    "encoder = models.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "decoder = models.Model(decoder_inputs, decoder_outputs, name='decoder')\n",
    "\n",
    "outputs = decoder(encoder(encoder_inputs)[2])\n",
    "vae = models.Model(encoder_inputs, outputs, name='vae')\n",
    "\n",
    "# Loss\n",
    "def vae_loss(x, outputs):\n",
    "    x_decoded_mean = outputs\n",
    "    z_log_var = outputs[1]\n",
    "    z_mean = outputs[0]\n",
    "    \n",
    "    reconstruction_loss = tf.reduce_mean(tf.keras.losses.mean_squared_error(x, x_decoded_mean), axis=-1)\n",
    "    kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
    "    return tf.reduce_mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "vae.compile(optimizer=optimizer, loss=vae_loss)\n",
    "\n",
    "vae.fit(x_train, x_train, epochs=200, batch_size=64, validation_data=(x_test, x_test))\n",
    "\n",
    "latent_train = encoder.predict(x_train)[2]  # Extracting only the z vector\n",
    "reconstructed_data = decoder.predict(latent_train)\n",
    "latent_train_array = np.array(latent_train)\n",
    "sequence_length = latent_train_array.shape[1]\n",
    "print(\"Sequence Length:\", sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e52e765-0bcf-40a9-9472-8bc62c35e4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.25.2\n",
      "  Downloading numpy-1.25.2-cp311-cp311-win_amd64.whl.metadata (5.7 kB)\n",
      "Downloading numpy-1.25.2-cp311-cp311-win_amd64.whl (15.5 MB)\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/15.5 MB 162.5 kB/s eta 0:01:36\n",
      "   ---------------------------------------- 0.0/15.5 MB 245.8 kB/s eta 0:01:04\n",
      "   ---------------------------------------- 0.1/15.5 MB 490.2 kB/s eta 0:00:32\n",
      "    --------------------------------------- 0.4/15.5 MB 1.5 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.2/15.5 MB 4.4 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 2.6/15.5 MB 8.8 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.6/15.5 MB 7.0 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.7/15.5 MB 9.1 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 4.1/15.5 MB 8.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 4.1/15.5 MB 8.8 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 4.5/15.5 MB 8.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 4.8/15.5 MB 8.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 4.9/15.5 MB 8.0 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 5.1/15.5 MB 7.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 5.4/15.5 MB 7.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 5.5/15.5 MB 7.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 5.9/15.5 MB 7.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 6.1/15.5 MB 7.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 6.1/15.5 MB 7.0 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 6.5/15.5 MB 6.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 6.7/15.5 MB 6.6 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 7.0/15.5 MB 6.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 7.2/15.5 MB 6.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.4/15.5 MB 6.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.6/15.5 MB 6.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.7/15.5 MB 6.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 7.9/15.5 MB 6.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 8.0/15.5 MB 6.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 8.2/15.5 MB 5.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 8.3/15.5 MB 5.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 8.4/15.5 MB 5.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 8.5/15.5 MB 5.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 8.5/15.5 MB 5.5 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 8.6/15.5 MB 5.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 8.7/15.5 MB 5.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 8.8/15.5 MB 5.1 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 8.9/15.5 MB 5.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 9.0/15.5 MB 5.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 9.1/15.5 MB 4.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 9.2/15.5 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 9.3/15.5 MB 4.8 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.4/15.5 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.5/15.5 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.6/15.5 MB 4.6 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 9.7/15.5 MB 4.6 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 9.8/15.5 MB 4.5 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 10.0/15.5 MB 4.5 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 10.1/15.5 MB 4.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 10.2/15.5 MB 4.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 10.4/15.5 MB 4.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 10.5/15.5 MB 4.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 10.6/15.5 MB 4.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 10.8/15.5 MB 4.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 10.9/15.5 MB 4.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 11.1/15.5 MB 4.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 11.3/15.5 MB 4.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.4/15.5 MB 4.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.6/15.5 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.7/15.5 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.9/15.5 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.1/15.5 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.2/15.5 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.4/15.5 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.6/15.5 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.8/15.5 MB 3.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.0/15.5 MB 3.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.1/15.5 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.4/15.5 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.5/15.5 MB 3.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.7/15.5 MB 3.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.9/15.5 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.1/15.5 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.3/15.5 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.6/15.5 MB 3.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.8/15.5 MB 3.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.0/15.5 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.2/15.5 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.5/15.5 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.5/15.5 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.5/15.5 MB 3.4 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "Successfully installed numpy-1.25.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\dwill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\~~mpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\dwill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\~~mpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jax 0.7.0 requires ml_dtypes>=0.5.0, but you have ml-dtypes 0.2.0 which is incompatible.\n",
      "jax 0.7.0 requires numpy>=1.26, but you have numpy 1.25.2 which is incompatible.\n",
      "jaxlib 0.7.0 requires ml_dtypes>=0.5.0, but you have ml-dtypes 0.2.0 which is incompatible.\n",
      "jaxlib 0.7.0 requires numpy>=1.26, but you have numpy 1.25.2 which is incompatible.\n",
      "scikit-criteria 0.9 requires numpy>=2.0, but you have numpy 1.25.2 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install numpy==1.25.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2c59a54-d4ea-4143-8eca-c10c749996bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 1s 7ms/step\n",
      "Mean Reconstruction Error for test data: 0.0261\n",
      "196/196 [==============================] - 2s 9ms/step\n",
      "Mean Reconstruction Error for train data: 0.0256\n"
     ]
    }
   ],
   "source": [
    "# reconstruction loss\n",
    "reconstructed_test = vae.predict(x_test)\n",
    "reconstruction_errors_test = np.mean(np.square(x_test - reconstructed_test), axis=1)\n",
    "print(f\"Mean Reconstruction Error for test data: {np.mean(reconstruction_errors_test):.4f}\")\n",
    "reconstructed_train = vae.predict(x_train)\n",
    "reconstruction_errors_train = np.mean(np.square(x_train - reconstructed_train), axis=1)\n",
    "print(f\"Mean Reconstruction Error for train data: {np.mean(reconstruction_errors_train):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2abe8832-cd0c-42be-aee3-6321393264ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.0256325597745432\n",
      "Root Mean Squared Error (RMSE): 0.16010171696313316\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "original_data_array = x_train.values\n",
    "original_data_flat = original_data_array.flatten()\n",
    "reconstructed_data_flat = reconstructed_data.flatten()\n",
    "mse = mean_squared_error(original_data_flat, reconstructed_data_flat)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea33b1d2-61e9-4247-b5ca-1135c7d35176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6255, 6, 1)\n"
     ]
    }
   ],
   "source": [
    "latent_train_reshaped = latent_train_array.reshape(latent_train_array.shape[0], latent_train_array.shape[1],1)\n",
    "print(latent_train_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29287050-a27e-4226-b02f-b2c5ba759815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 8ms/step\n",
      "(696, 6, 1)\n"
     ]
    }
   ],
   "source": [
    "latent_test = encoder.predict(x_test)[2]\n",
    "latent_test_array = np.array(latent_test)\n",
    "latent_test_reshaped = latent_test_array.reshape(latent_test_array.shape[0], latent_test_array.shape[1],1)\n",
    "print(latent_test_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14b523cb-0203-4fd5-9960-d344210af462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Fold 1...\n",
      "Epoch 1/10\n",
      "695/695 [==============================] - 20s 19ms/step - loss: 0.0982\n",
      "Epoch 2/10\n",
      "695/695 [==============================] - 13s 19ms/step - loss: 0.0276\n",
      "Epoch 3/10\n",
      "695/695 [==============================] - 13s 19ms/step - loss: 0.0101\n",
      "Epoch 4/10\n",
      "695/695 [==============================] - 13s 19ms/step - loss: 0.0067\n",
      "Epoch 5/10\n",
      "695/695 [==============================] - 14s 20ms/step - loss: 0.0050\n",
      "Epoch 6/10\n",
      "695/695 [==============================] - 13s 19ms/step - loss: 0.0043\n",
      "Epoch 7/10\n",
      "695/695 [==============================] - 13s 19ms/step - loss: 0.0038\n",
      "Epoch 8/10\n",
      "695/695 [==============================] - 14s 19ms/step - loss: 0.0034\n",
      "Epoch 9/10\n",
      "695/695 [==============================] - 14s 20ms/step - loss: 0.0032\n",
      "Epoch 10/10\n",
      "695/695 [==============================] - 14s 20ms/step - loss: 0.0030\n",
      "44/44 [==============================] - 2s 17ms/step - loss: 0.0018\n",
      "Fold 1 - Test Loss: 0.001774831092916429\n",
      "44/44 [==============================] - 2s 16ms/step\n",
      "Fold 1 - Mean Squared Error (MSE): 0.0017748309773273452\n",
      "Fold 1 - Mean Absolute Error (MAE): 0.030488035349970844\n",
      "Fold 1 - R-squared (R2) Score: 0.983445029533257\n",
      "Training on Fold 2...\n",
      "Epoch 1/10\n",
      "696/696 [==============================] - 23s 22ms/step - loss: 0.0974\n",
      "Epoch 2/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0272\n",
      "Epoch 3/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0090\n",
      "Epoch 4/10\n",
      "696/696 [==============================] - 16s 23ms/step - loss: 0.0061\n",
      "Epoch 5/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0052\n",
      "Epoch 6/10\n",
      "696/696 [==============================] - 16s 23ms/step - loss: 0.0042\n",
      "Epoch 7/10\n",
      "696/696 [==============================] - 16s 22ms/step - loss: 0.0038\n",
      "Epoch 8/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0034\n",
      "Epoch 9/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0032\n",
      "Epoch 10/10\n",
      "696/696 [==============================] - 16s 22ms/step - loss: 0.0030\n",
      "44/44 [==============================] - 2s 16ms/step - loss: 0.0019\n",
      "Fold 2 - Test Loss: 0.001944929244928062\n",
      "44/44 [==============================] - 2s 15ms/step\n",
      "Fold 2 - Mean Squared Error (MSE): 0.0019449290444109799\n",
      "Fold 2 - Mean Absolute Error (MAE): 0.03280562460595971\n",
      "Fold 2 - R-squared (R2) Score: 0.9821028744585871\n",
      "Training on Fold 3...\n",
      "Epoch 1/10\n",
      "696/696 [==============================] - 23s 22ms/step - loss: 0.0958\n",
      "Epoch 2/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0294\n",
      "Epoch 3/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0110\n",
      "Epoch 4/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0068\n",
      "Epoch 5/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0054\n",
      "Epoch 6/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0047\n",
      "Epoch 7/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0040\n",
      "Epoch 8/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0036\n",
      "Epoch 9/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0033\n",
      "Epoch 10/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0031\n",
      "44/44 [==============================] - 2s 16ms/step - loss: 0.0017\n",
      "Fold 3 - Test Loss: 0.001692727324552834\n",
      "44/44 [==============================] - 1s 8ms/step\n",
      "Fold 3 - Mean Squared Error (MSE): 0.001692727500446537\n",
      "Fold 3 - Mean Absolute Error (MAE): 0.030926577737206674\n",
      "Fold 3 - R-squared (R2) Score: 0.983975662750954\n",
      "Training on Fold 4...\n",
      "Epoch 1/10\n",
      "696/696 [==============================] - 22s 22ms/step - loss: 0.1010\n",
      "Epoch 2/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0253\n",
      "Epoch 3/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0100\n",
      "Epoch 4/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0067\n",
      "Epoch 5/10\n",
      "696/696 [==============================] - 16s 22ms/step - loss: 0.0052\n",
      "Epoch 6/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0045\n",
      "Epoch 7/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0040\n",
      "Epoch 8/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0036\n",
      "Epoch 9/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0032\n",
      "Epoch 10/10\n",
      "696/696 [==============================] - 16s 23ms/step - loss: 0.0030\n",
      "44/44 [==============================] - 2s 15ms/step - loss: 0.0023\n",
      "Fold 4 - Test Loss: 0.0023476583883166313\n",
      "44/44 [==============================] - 2s 15ms/step\n",
      "Fold 4 - Mean Squared Error (MSE): 0.0023476580845605117\n",
      "Fold 4 - Mean Absolute Error (MAE): 0.03574790777340558\n",
      "Fold 4 - R-squared (R2) Score: 0.9773638242155702\n",
      "Training on Fold 5...\n",
      "Epoch 1/10\n",
      "696/696 [==============================] - 24s 22ms/step - loss: 0.0909\n",
      "Epoch 2/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0234\n",
      "Epoch 3/10\n",
      "696/696 [==============================] - 16s 22ms/step - loss: 0.0090\n",
      "Epoch 4/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0063\n",
      "Epoch 5/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0051\n",
      "Epoch 6/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0044\n",
      "Epoch 7/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0038\n",
      "Epoch 8/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0035\n",
      "Epoch 9/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0033\n",
      "Epoch 10/10\n",
      "696/696 [==============================] - 15s 22ms/step - loss: 0.0030\n",
      "44/44 [==============================] - 2s 18ms/step - loss: 0.0019\n",
      "Fold 5 - Test Loss: 0.0019383025355637074\n",
      "44/44 [==============================] - 2s 16ms/step\n",
      "Fold 5 - Mean Squared Error (MSE): 0.0019383023967931474\n",
      "Fold 5 - Mean Absolute Error (MAE): 0.03253356844590174\n",
      "Fold 5 - R-squared (R2) Score: 0.981408898511844\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Prepare data\n",
    "latent_data = np.concatenate((latent_train_reshaped, latent_test_reshaped), axis=0)\n",
    "target_data = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold = 1\n",
    "for train_index, test_index in kf.split(latent_data):\n",
    "    print(f\"Training on Fold {fold}...\")\n",
    "    \n",
    "    x_train_fold, x_test_fold = latent_data[train_index], latent_data[test_index]\n",
    "    y_train_fold, y_test_fold = target_data[train_index], target_data[test_index]\n",
    "    \n",
    "    # Define LSTM model\n",
    "    vae_lstm = Sequential()\n",
    "    vae_lstm.add(LSTM(units=150, activation='relu', return_sequences=True, input_shape=(6, 1)))\n",
    "    vae_lstm.add(Dropout(0.2))\n",
    "    vae_lstm.add(LSTM(units=150, activation='relu'))\n",
    "    vae_lstm.add(Dense(units=1))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    vae_lstm.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    # Train model\n",
    "    vae_lstm.fit(x_train_fold, y_train_fold, epochs=10, batch_size=8, verbose=1)\n",
    "    \n",
    "    # Evaluate model\n",
    "    loss = vae_lstm.evaluate(x_test_fold, y_test_fold)\n",
    "    print(f\"Fold {fold} - Test Loss: {loss}\")\n",
    "    \n",
    "    # Predictions and metrics\n",
    "    y_pred = vae_lstm.predict(x_test_fold)\n",
    "    mse = mean_squared_error(y_test_fold, y_pred)\n",
    "    mae = mean_absolute_error(y_test_fold, y_pred)\n",
    "    r2 = r2_score(y_test_fold, y_pred)\n",
    "    \n",
    "    print(f\"Fold {fold} - Mean Squared Error (MSE):\", mse)\n",
    "    print(f\"Fold {fold} - Mean Absolute Error (MAE):\", mae)\n",
    "    print(f\"Fold {fold} - R-squared (R2) Score:\", r2)\n",
    "    \n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29d24c6e-5341-454d-8c0d-a9d943dba14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_lstm.save('vae_lstm.keras')\n",
    "# from tensorflow.keras.models import load_model\n",
    "# vae_lstm = load_model('vae_lstm.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871ac696-6ab4-48b4-8f0b-acd227ad739a",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95d38527-0b50-45b0-8291-9c4608213d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6255, 6, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.values\n",
    "y_train = y_train.values\n",
    "feature=1\n",
    "x_train = x_train.reshape((x_train.shape[0],x_train.shape[1],feature))\n",
    "print(x_train.shape)\n",
    "x_test = x_test.values\n",
    "y_test = y_test.values\n",
    "x_test = x_test.reshape((x_test.shape[0],x_test.shape[1],feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "894ac223-5eb6-48ae-984d-448bbea32bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 25s 22ms/step - loss: 0.0884\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 18s 22ms/step - loss: 0.0317\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.0113\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.0081\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.0068\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.0058\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.0052\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.0046\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.0041\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.0037\n",
      "22/22 [==============================] - 1s 16ms/step - loss: 0.0017\n",
      "Test Loss: 0.0016879051690921187\n",
      "22/22 [==============================] - 1s 16ms/step\n",
      "Mean Squared Error (MSE): 0.0016879051589465792\n",
      "Mean Absolute Error (MAE): 0.0279346169723211\n",
      "R-squared (R2) Score: 0.9841340848475054\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "lstm = Sequential()\n",
    "lstm.add(LSTM(units=150, activation='relu', return_sequences=True, input_shape=(6,1)))\n",
    "lstm.add(Dropout(0.2))\n",
    "lstm.add(LSTM(units=150, activation='relu'))\n",
    "lstm.add(Dense(units=1))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "lstm.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "lstm.fit(x_train, y_train, epochs=10, batch_size=8)\n",
    "loss = lstm.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "y_pred = lstm.predict(x_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared (R2) Score:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c865b8-922f-46f4-9d2f-e3c7c2eb4894",
   "metadata": {},
   "source": [
    "# TOPSIS METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20ef1477-b0f1-48f3-ba9a-7cf7ed46b16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n",
      "Package                          Version\n",
      "-------------------------------- ------------\n",
      "absl-py                          2.3.1\n",
      "aiohttp                          3.9.5\n",
      "aiosignal                        1.2.0\n",
      "anyio                            4.0.0\n",
      "argon2-cffi                      23.1.0\n",
      "argon2-cffi-bindings             21.2.0\n",
      "arrow                            1.3.0\n",
      "asgiref                          3.8.1\n",
      "asttokens                        2.4.0\n",
      "astunparse                       1.6.3\n",
      "async-lru                        2.0.4\n",
      "async-timeout                    4.0.2\n",
      "attrs                            25.3.0\n",
      "Babel                            2.13.0\n",
      "backcall                         0.2.0\n",
      "base58                           2.1.1\n",
      "beautifulsoup4                   4.12.2\n",
      "bitarray                         2.9.2\n",
      "bleach                           6.1.0\n",
      "cachetools                       6.2.0\n",
      "certifi                          2021.10.8\n",
      "cffi                             1.16.0\n",
      "charset-normalizer               2.0.12\n",
      "click                            8.3.0\n",
      "cloudinary                       1.29.0\n",
      "colorama                         0.4.6\n",
      "comm                             0.1.4\n",
      "contourpy                        1.3.3\n",
      "crispy-bootstrap5                0.6\n",
      "cssselect                        1.3.0\n",
      "custom-inherit                   2.4.1\n",
      "cycler                           0.12.1\n",
      "cytoolz                          0.12.3\n",
      "debugpy                          1.8.0\n",
      "decorator                        5.1.1\n",
      "defusedxml                       0.7.1\n",
      "Deprecated                       1.2.18\n",
      "Django                           4.0\n",
      "django-bootstrap-datepicker-plus 4.0.0\n",
      "django-cloudinary-storage        0.3.0\n",
      "django-crispy-forms              1.14.0\n",
      "dnspython                        2.6.1\n",
      "eth-abi                          2.1.1\n",
      "eth-account                      0.5.9\n",
      "eth-hash                         0.3.2\n",
      "eth-keyfile                      0.5.1\n",
      "eth-keys                         0.3.4\n",
      "eth-rlp                          0.2.1\n",
      "eth-typing                       2.3.0\n",
      "eth-utils                        1.10.0\n",
      "executing                        2.0.0\n",
      "fastjsonschema                   2.18.1\n",
      "filelock                         3.13.1\n",
      "flatbuffers                      25.2.10\n",
      "fonttools                        4.43.1\n",
      "fqdn                             1.5.1\n",
      "frozenlist                       1.4.1\n",
      "fsspec                           2024.2.0\n",
      "gast                             0.6.0\n",
      "google-auth                      2.41.1\n",
      "google-auth-oauthlib             1.2.2\n",
      "google-pasta                     0.2.0\n",
      "grpcio                           1.75.1\n",
      "gunicorn                         20.1.0\n",
      "h5py                             3.14.0\n",
      "hexbytes                         0.2.2\n",
      "idna                             3.3\n",
      "importlib-resources              5.7.1\n",
      "instaloader                      4.10.2\n",
      "ipfshttpclient                   0.8.0a2\n",
      "ipykernel                        6.25.2\n",
      "ipython                          8.16.1\n",
      "ipython-genutils                 0.2.0\n",
      "ipywidgets                       8.1.1\n",
      "isoduration                      20.11.0\n",
      "jax                              0.7.0\n",
      "jaxlib                           0.7.0\n",
      "jedi                             0.19.1\n",
      "Jinja2                           3.1.2\n",
      "joblib                           1.5.2\n",
      "json5                            0.9.14\n",
      "jsonpointer                      2.4\n",
      "jsonschema                       4.25.1\n",
      "jsonschema-specifications        2023.7.1\n",
      "jupyter                          1.0.0\n",
      "jupyter_client                   8.4.0\n",
      "jupyter-console                  6.6.3\n",
      "jupyter_core                     5.4.0\n",
      "jupyter-events                   0.8.0\n",
      "jupyter-lsp                      2.2.0\n",
      "jupyter_server                   2.8.0\n",
      "jupyter_server_terminals         0.4.4\n",
      "jupyterlab                       4.0.7\n",
      "jupyterlab-pygments              0.2.2\n",
      "jupyterlab_server                2.25.0\n",
      "jupyterlab-widgets               3.0.9\n",
      "kaggle                           1.7.4.5\n",
      "keras                            2.15.0\n",
      "kiwisolver                       1.4.5\n",
      "lark                             1.3.0\n",
      "libclang                         18.1.1\n",
      "lru-dict                         1.3.0\n",
      "lxml                             6.0.2\n",
      "Markdown                         3.9\n",
      "markdown-it-py                   4.0.0\n",
      "MarkupSafe                       2.1.3\n",
      "matplotlib                       3.8.4\n",
      "matplotlib-inline                0.1.6\n",
      "mdurl                            0.1.2\n",
      "mediapipe                        0.10.21\n",
      "methodtools                      0.4.7\n",
      "mistune                          3.0.2\n",
      "ml-dtypes                        0.2.0\n",
      "MouseInfo                        0.1.3\n",
      "mpmath                           1.3.0\n",
      "multiaddr                        0.0.9\n",
      "multidict                        6.0.5\n",
      "namex                            0.1.0\n",
      "nbclient                         0.8.0\n",
      "nbconvert                        7.9.2\n",
      "nbformat                         5.9.2\n",
      "nest-asyncio                     1.5.8\n",
      "netaddr                          0.8.0\n",
      "networkx                         3.2.1\n",
      "notebook                         7.0.6\n",
      "notebook_shim                    0.2.3\n",
      "numpy                            1.25.2\n",
      "oauthlib                         3.3.1\n",
      "opencv-contrib-python            4.11.0.86\n",
      "opencv-python                    4.8.1.78\n",
      "opendatasets                     0.1.22\n",
      "opt_einsum                       3.4.0\n",
      "optree                           0.17.0\n",
      "overrides                        7.4.0\n",
      "packaging                        23.2\n",
      "pandas                           2.3.3\n",
      "pandocfilters                    1.5.0\n",
      "parsimonious                     0.10.0\n",
      "parso                            0.8.3\n",
      "patsy                            0.5.6\n",
      "pickleshare                      0.7.5\n",
      "Pillow                           10.1.0\n",
      "pip                              24.0\n",
      "platformdirs                     3.11.0\n",
      "prometheus-client                0.17.1\n",
      "prompt-toolkit                   3.0.39\n",
      "protobuf                         4.25.8\n",
      "psutil                           5.9.6\n",
      "psycopg2                         2.9.9\n",
      "PuLP                             2.8.0\n",
      "pure-eval                        0.2.2\n",
      "py-solc-x                        1.1.1\n",
      "pyasn1                           0.6.1\n",
      "pyasn1_modules                   0.4.2\n",
      "PyAutoGUI                        0.9.54\n",
      "pycparser                        2.21\n",
      "pycryptodome                     3.14.1\n",
      "pygame                           2.5.2\n",
      "PyGetWindow                      0.0.9\n",
      "Pygments                         2.16.1\n",
      "pymongo                          4.8.0\n",
      "PyMsgBox                         1.0.9\n",
      "pyparsing                        3.1.1\n",
      "pyperclip                        1.8.2\n",
      "pypiwin32                        223\n",
      "pyquery                          2.0.1\n",
      "PyRect                           0.2.0\n",
      "pyrsistent                       0.18.1\n",
      "PyScreeze                        0.1.30\n",
      "python-dateutil                  2.8.2\n",
      "python-dotenv                    0.20.0\n",
      "python-json-logger               2.0.7\n",
      "python-slugify                   8.0.4\n",
      "pytweening                       1.0.7\n",
      "pytz                             2023.3.post1\n",
      "pywin32                          306\n",
      "pywinpty                         2.0.12\n",
      "PyYAML                           6.0.1\n",
      "pyzmq                            25.1.1\n",
      "qtconsole                        5.4.4\n",
      "QtPy                             2.4.0\n",
      "referencing                      0.30.2\n",
      "regex                            2024.4.28\n",
      "requests                         2.32.5\n",
      "requests-oauthlib                2.0.0\n",
      "rfc3339-validator                0.1.4\n",
      "rfc3986-validator                0.1.1\n",
      "rfc3987-syntax                   1.1.0\n",
      "rich                             14.1.0\n",
      "rlp                              2.0.1\n",
      "rpds-py                          0.10.6\n",
      "rsa                              4.9.1\n",
      "scikit-criteria                  0.9\n",
      "scikit-learn                     1.7.2\n",
      "scipy                            1.13.0\n",
      "screen_brightness_control        0.24.2\n",
      "seaborn                          0.13.0\n",
      "semantic-version                 2.9.0\n",
      "Send2Trash                       1.8.2\n",
      "sentencepiece                    0.2.0\n",
      "setuptools                       65.5.0\n",
      "six                              1.16.0\n",
      "sniffio                          1.3.0\n",
      "solc                             0.0.0a0\n",
      "sounddevice                      0.5.2\n",
      "soupsieve                        2.5\n",
      "sqlparse                         0.4.2\n",
      "stack-data                       0.6.3\n",
      "statsmodels                      0.14.2\n",
      "sympy                            1.12\n",
      "tensorboard                      2.15.2\n",
      "tensorboard-data-server          0.7.2\n",
      "tensorflow                       2.15.0\n",
      "tensorflow-estimator             2.15.0\n",
      "tensorflow-intel                 2.15.0\n",
      "tensorflow-io-gcs-filesystem     0.31.0\n",
      "termcolor                        3.1.0\n",
      "terminado                        0.17.1\n",
      "text-unidecode                   1.3\n",
      "threadpoolctl                    3.5.0\n",
      "tinycss2                         1.2.1\n",
      "toolz                            0.11.2\n",
      "torch                            2.4.1+cu124\n",
      "torchaudio                       2.4.1+cu124\n",
      "torchvision                      0.19.1+cu124\n",
      "tornado                          6.3.3\n",
      "tqdm                             4.67.1\n",
      "traitlets                        5.11.2\n",
      "types-python-dateutil            2.8.19.14\n",
      "typing_extensions                4.15.0\n",
      "tzdata                           2023.3\n",
      "uri-template                     1.3.0\n",
      "urllib3                          1.26.9\n",
      "varint                           1.0.2\n",
      "wcwidth                          0.2.8\n",
      "web3                             5.29.0\n",
      "webcolors                        24.11.1\n",
      "webencodings                     0.5.1\n",
      "websocket-client                 1.6.4\n",
      "websockets                       9.1\n",
      "Werkzeug                         3.1.3\n",
      "wheel                            0.45.1\n",
      "whitenoise                       6.0.0\n",
      "widgetsnbextension               4.0.9\n",
      "wirerope                         1.0.0\n",
      "WMI                              1.5.1\n",
      "wrapt                            1.14.2\n",
      "yarl                             1.9.4\n",
      "zipp                             3.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30213efa-4653-4209-9ab0-e4eff38d3b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-criteria==0.7 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.7)\n",
      "Requirement already satisfied: numpy in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-criteria==0.7) (2.2.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-criteria==0.7) (2.3.3)\n",
      "Requirement already satisfied: pyquery in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-criteria==0.7) (2.0.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-criteria==0.7) (1.13.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-criteria==0.7) (3.1.2)\n",
      "Requirement already satisfied: custom-inherit in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-criteria==0.7) (2.4.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-criteria==0.7) (0.13.0)\n",
      "Requirement already satisfied: pulp in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-criteria==0.7) (2.8.0)\n",
      "Requirement already satisfied: Deprecated in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-criteria==0.7) (1.2.18)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Deprecated->scikit-criteria==0.7) (1.14.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->scikit-criteria==0.7) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->scikit-criteria==0.7) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->scikit-criteria==0.7) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->scikit-criteria==0.7) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->scikit-criteria==0.7) (1.16.0)\n",
      "Requirement already satisfied: lxml>=2.1 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyquery->scikit-criteria==0.7) (6.0.2)\n",
      "Requirement already satisfied: cssselect>=1.2.0 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyquery->scikit-criteria==0.7) (1.3.0)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.3 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from seaborn->scikit-criteria==0.7) (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn->scikit-criteria==0.7) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn->scikit-criteria==0.7) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn->scikit-criteria==0.7) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn->scikit-criteria==0.7) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn->scikit-criteria==0.7) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn->scikit-criteria==0.7) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn->scikit-criteria==0.7) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "# !pip install scikit-criteria==0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca696001-ce8a-4440-8b88-c42816f9d14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-criteria in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.7)\n",
      "Collecting scikit-criteria\n",
      "  Using cached scikit_criteria-0.9-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: numpy>=2.0 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-criteria) (2.2.6)\n",
      "Requirement already satisfied: networkx>=3.2 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-criteria) (3.2.1)\n",
      "Requirement already satisfied: pandas>=2.0 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-criteria) (2.3.3)\n",
      "Requirement already satisfied: scipy>=1.10 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-criteria) (1.13.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-criteria) (3.1.2)\n",
      "Requirement already satisfied: custom_inherit in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-criteria) (2.4.1)\n",
      "Requirement already satisfied: seaborn<0.14,>=0.13 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-criteria) (0.13.0)\n",
      "Requirement already satisfied: pulp<2.9,>=2.8 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-criteria) (2.8.0)\n",
      "Requirement already satisfied: Deprecated in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-criteria) (1.2.18)\n",
      "Requirement already satisfied: scikit-learn>=1.6 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-criteria) (1.7.2)\n",
      "Requirement already satisfied: matplotlib<3.9,>=3.8.2 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-criteria) (3.8.4)\n",
      "Requirement already satisfied: methodtools in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-criteria) (0.4.7)\n",
      "Requirement already satisfied: pyyaml>=6.0 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-criteria) (6.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib<3.9,>=3.8.2->scikit-criteria) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib<3.9,>=3.8.2->scikit-criteria) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib<3.9,>=3.8.2->scikit-criteria) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib<3.9,>=3.8.2->scikit-criteria) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib<3.9,>=3.8.2->scikit-criteria) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib<3.9,>=3.8.2->scikit-criteria) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib<3.9,>=3.8.2->scikit-criteria) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib<3.9,>=3.8.2->scikit-criteria) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=2.0->scikit-criteria) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=2.0->scikit-criteria) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib<3.9,>=3.8.2->scikit-criteria) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=1.6->scikit-criteria) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=1.6->scikit-criteria) (3.5.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Deprecated->scikit-criteria) (1.14.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->scikit-criteria) (2.1.3)\n",
      "Requirement already satisfied: wirerope>=0.4.7 in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from methodtools->scikit-criteria) (1.0.0)\n",
      "Using cached scikit_criteria-0.9-py3-none-any.whl (176 kB)\n",
      "Installing collected packages: scikit-criteria\n",
      "  Attempting uninstall: scikit-criteria\n",
      "    Found existing installation: scikit-criteria 0.7\n",
      "    Uninstalling scikit-criteria-0.7:\n",
      "      Successfully uninstalled scikit-criteria-0.7\n",
      "Successfully installed scikit-criteria-0.9\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade scikit-criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ad1104a-f371-4371-9f54-41eac64b2ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0', '9')\n"
     ]
    }
   ],
   "source": [
    "# import skcriteria\n",
    "# print(skcriteria.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a9b006d-b690-41f2-9003-0fbe1b5a2cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Package(s) not found: skcriteria\n"
     ]
    }
   ],
   "source": [
    "# !pip install scikit-criteria\n",
    "!pip show skcriteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08d2b41c-f3e9-4883-9fdd-edb208060a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\dwill\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (24.0)\n",
      "Collecting pip\n",
      "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.8 MB 435.7 kB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.1/1.8 MB 871.5 kB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.5/1.8 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.1/1.8 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.8/1.8 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 7.4 MB/s eta 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\dwill\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ee6fc6-66b4-4f54-ad71-bef514ab9c1f",
   "metadata": {},
   "source": [
    "# Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ec00726-47f0-4097-894a-8f273c3bbed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOPSIS Feature Ranking:\n",
      "   Feature  Closeness  Rank\n",
      "0     Wdir   0.457240   1.0\n",
      "1     Itmp   0.450577   2.0\n",
      "2     Prtv   0.450134   3.0\n",
      "3     Etmp   0.449717   4.0\n",
      "4     Ndir   0.444554   5.0\n",
      "5     Wspd   0.438209   6.0\n",
      "6      Day   0.435778   7.0\n",
      "7     hour   0.428405   8.0\n",
      "8   minute   0.422750   9.0\n",
      "9     Pab1   0.417262  10.0\n",
      "10    Pab3   0.417256  11.0\n",
      "11    Pab2   0.398499  12.0\n",
      "12  TurbID   0.000000  13.0\n",
      "\n",
      "Selected Features via TOPSIS:\n",
      "       Wdir      Itmp      Prtv      Etmp      Ndir      Wspd\n",
      "0  0.310772  0.625951  0.500000  0.611111  0.271912  0.364055\n",
      "1  0.397045  0.619790  0.541667  0.606421  0.266146  0.370639\n",
      "2  0.466158  0.615803  0.458333  0.603535  0.266146  0.380513\n",
      "3  0.543375  0.610729  0.583333  0.602453  0.266146  0.369322\n",
      "4  0.451859  0.604929  0.416667  0.601732  0.266146  0.359447\n",
      "WARNING:tensorflow:From C:\\Users\\dwill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Epoch 1/50\n",
      "98/98 [==============================] - 9s 34ms/step - loss: 0.0323 - val_loss: 0.0303\n",
      "Epoch 2/50\n",
      "98/98 [==============================] - 1s 14ms/step - loss: 0.0316 - val_loss: 0.0303\n",
      "Epoch 3/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0317 - val_loss: 0.0302\n",
      "Epoch 4/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0316 - val_loss: 0.0301\n",
      "Epoch 5/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0316 - val_loss: 0.0305\n",
      "Epoch 6/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0316 - val_loss: 0.0301\n",
      "Epoch 7/50\n",
      "98/98 [==============================] - 1s 14ms/step - loss: 0.0315 - val_loss: 0.0301\n",
      "Epoch 8/50\n",
      "98/98 [==============================] - 1s 14ms/step - loss: 0.0315 - val_loss: 0.0302\n",
      "Epoch 9/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0300\n",
      "Epoch 10/50\n",
      "98/98 [==============================] - 2s 16ms/step - loss: 0.0316 - val_loss: 0.0301\n",
      "Epoch 11/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0301\n",
      "Epoch 12/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0301\n",
      "Epoch 13/50\n",
      "98/98 [==============================] - 1s 14ms/step - loss: 0.0315 - val_loss: 0.0300\n",
      "Epoch 14/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0301\n",
      "Epoch 15/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0300\n",
      "Epoch 16/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0301\n",
      "Epoch 17/50\n",
      "98/98 [==============================] - 1s 14ms/step - loss: 0.0315 - val_loss: 0.0300\n",
      "Epoch 18/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0301\n",
      "Epoch 19/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0300\n",
      "Epoch 20/50\n",
      "98/98 [==============================] - 1s 14ms/step - loss: 0.0315 - val_loss: 0.0302\n",
      "Epoch 21/50\n",
      "98/98 [==============================] - 1s 14ms/step - loss: 0.0315 - val_loss: 0.0300\n",
      "Epoch 22/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0301\n",
      "Epoch 23/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0300\n",
      "Epoch 24/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0300\n",
      "Epoch 25/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0300\n",
      "Epoch 26/50\n",
      "98/98 [==============================] - 1s 14ms/step - loss: 0.0315 - val_loss: 0.0301\n",
      "Epoch 27/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0300\n",
      "Epoch 28/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0300\n",
      "Epoch 29/50\n",
      "98/98 [==============================] - 1s 14ms/step - loss: 0.0315 - val_loss: 0.0300\n",
      "Epoch 30/50\n",
      "98/98 [==============================] - 1s 14ms/step - loss: 0.0315 - val_loss: 0.0301\n",
      "Epoch 31/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0301\n",
      "Epoch 32/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0301\n",
      "Epoch 33/50\n",
      "98/98 [==============================] - 2s 16ms/step - loss: 0.0315 - val_loss: 0.0301\n",
      "Epoch 34/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0300\n",
      "Epoch 35/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0300\n",
      "Epoch 36/50\n",
      "98/98 [==============================] - 1s 14ms/step - loss: 0.0315 - val_loss: 0.0300\n",
      "Epoch 37/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0300\n",
      "Epoch 38/50\n",
      "98/98 [==============================] - 1s 14ms/step - loss: 0.0315 - val_loss: 0.0300\n",
      "Epoch 39/50\n",
      "98/98 [==============================] - 1s 14ms/step - loss: 0.0315 - val_loss: 0.0300\n",
      "Epoch 40/50\n",
      "98/98 [==============================] - 2s 15ms/step - loss: 0.0315 - val_loss: 0.0301\n",
      "Epoch 41/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0300\n",
      "Epoch 42/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0300\n",
      "Epoch 43/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0300\n",
      "Epoch 44/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0300\n",
      "Epoch 45/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0301\n",
      "Epoch 46/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0300\n",
      "Epoch 47/50\n",
      "98/98 [==============================] - 2s 16ms/step - loss: 0.0315 - val_loss: 0.0299\n",
      "Epoch 48/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0300\n",
      "Epoch 49/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0301\n",
      "Epoch 50/50\n",
      "98/98 [==============================] - 1s 15ms/step - loss: 0.0315 - val_loss: 0.0301\n",
      "196/196 [==============================] - 1s 5ms/step\n",
      "22/22 [==============================] - 0s 5ms/step\n",
      "\n",
      "Training on Fold 1...\n",
      "Epoch 1/10\n",
      "348/348 [==============================] - 19s 22ms/step - loss: 0.1380\n",
      "Epoch 2/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1067\n",
      "Epoch 3/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1064\n",
      "Epoch 4/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1061\n",
      "Epoch 5/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1063\n",
      "Epoch 6/10\n",
      "348/348 [==============================] - 8s 22ms/step - loss: 0.1063\n",
      "Epoch 7/10\n",
      "348/348 [==============================] - 8s 22ms/step - loss: 0.1060\n",
      "Epoch 8/10\n",
      "348/348 [==============================] - 8s 22ms/step - loss: 0.1063\n",
      "Epoch 9/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1061\n",
      "Epoch 10/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1061\n",
      "Fold 1 - Test Loss: 0.10714451968669891\n",
      "44/44 [==============================] - 3s 11ms/step\n",
      "Fold 1 - MSE: 0.10714453356964913\n",
      "Fold 1 - MAE: 0.28666736103991575\n",
      "Fold 1 - R2: 0.0005952050771842465\n",
      "\n",
      "Training on Fold 2...\n",
      "Epoch 1/10\n",
      "348/348 [==============================] - 20s 23ms/step - loss: 0.1366\n",
      "Epoch 2/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1059\n",
      "Epoch 3/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1061\n",
      "Epoch 4/10\n",
      "348/348 [==============================] - 8s 24ms/step - loss: 0.1059\n",
      "Epoch 5/10\n",
      "348/348 [==============================] - 8s 24ms/step - loss: 0.1057\n",
      "Epoch 6/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1057\n",
      "Epoch 7/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1058\n",
      "Epoch 8/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1056\n",
      "Epoch 9/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1058\n",
      "Epoch 10/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1057\n",
      "Fold 2 - Test Loss: 0.10947107523679733\n",
      "44/44 [==============================] - 3s 12ms/step\n",
      "Fold 2 - MSE: 0.10947106684802355\n",
      "Fold 2 - MAE: 0.28616992190708074\n",
      "Fold 2 - R2: -0.007346479894248192\n",
      "\n",
      "Training on Fold 3...\n",
      "Epoch 1/10\n",
      "348/348 [==============================] - 20s 23ms/step - loss: 0.1368\n",
      "Epoch 2/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1069\n",
      "Epoch 3/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1070\n",
      "Epoch 4/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1068\n",
      "Epoch 5/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1069\n",
      "Epoch 6/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1065\n",
      "Epoch 7/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1064\n",
      "Epoch 8/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1066\n",
      "Epoch 9/10\n",
      "348/348 [==============================] - 8s 22ms/step - loss: 0.1065\n",
      "Epoch 10/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1064\n",
      "Fold 3 - Test Loss: 0.10581991821527481\n",
      "44/44 [==============================] - 3s 11ms/step\n",
      "Fold 3 - MSE: 0.1058199115650401\n",
      "Fold 3 - MAE: 0.2831571428505006\n",
      "Fold 3 - R2: -0.001752467621100262\n",
      "\n",
      "Training on Fold 4...\n",
      "Epoch 1/10\n",
      "348/348 [==============================] - 20s 23ms/step - loss: 0.1398\n",
      "Epoch 2/10\n",
      "348/348 [==============================] - 8s 24ms/step - loss: 0.1074\n",
      "Epoch 3/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1076\n",
      "Epoch 4/10\n",
      "348/348 [==============================] - 8s 24ms/step - loss: 0.1074\n",
      "Epoch 5/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1075\n",
      "Epoch 6/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1070\n",
      "Epoch 7/10\n",
      "348/348 [==============================] - 8s 24ms/step - loss: 0.1072\n",
      "Epoch 8/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1072\n",
      "Epoch 9/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1073\n",
      "Epoch 10/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1069\n",
      "Fold 4 - Test Loss: 0.1047675833106041\n",
      "44/44 [==============================] - 2s 7ms/step\n",
      "Fold 4 - MSE: 0.10476757844014235\n",
      "Fold 4 - MAE: 0.28270910314334824\n",
      "Fold 4 - R2: -0.010171514189665887\n",
      "\n",
      "Training on Fold 5...\n",
      "Epoch 1/10\n",
      "348/348 [==============================] - 20s 23ms/step - loss: 0.1401\n",
      "Epoch 2/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1074\n",
      "Epoch 3/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1070\n",
      "Epoch 4/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1071\n",
      "Epoch 5/10\n",
      "348/348 [==============================] - 8s 24ms/step - loss: 0.1070\n",
      "Epoch 6/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1073\n",
      "Epoch 7/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1070\n",
      "Epoch 8/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1072\n",
      "Epoch 9/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1070\n",
      "Epoch 10/10\n",
      "348/348 [==============================] - 8s 23ms/step - loss: 0.1070\n",
      "Fold 5 - Test Loss: 0.10442811995744705\n",
      "44/44 [==============================] - 2s 7ms/step\n",
      "Fold 5 - MSE: 0.10442812001536558\n",
      "Fold 5 - MAE: 0.2790481957636156\n",
      "Fold 5 - R2: -0.0016155273991469432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dwill\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# updated_pipeline.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ===========================\n",
    "# Load & preprocess dataset\n",
    "# ===========================\n",
    "data = pd.read_csv(r\"C:\\Users\\dwill\\OneDrive\\Desktop\\INTERN WORKS\\wind_data.csv\")\n",
    "data = data.dropna()\n",
    "\n",
    "# Split Tmstamp\n",
    "data[['hour', 'minute']] = data['Tmstamp'].str.split(':', expand=True)\n",
    "data['hour'] = pd.to_numeric(data['hour'])\n",
    "data['minute'] = pd.to_numeric(data['minute'])\n",
    "data.drop(columns=['Tmstamp'], inplace=True)\n",
    "\n",
    "desired_order = [\n",
    "    'TurbID', 'Day', 'hour', 'minute','Wspd', 'Wdir', 'Etmp', 'Itmp', 'Ndir',\n",
    "    'Pab1', 'Pab2', 'Pab3', 'Prtv', 'Patv'\n",
    "]\n",
    "data = data.reindex(columns=desired_order)\n",
    "\n",
    "# Outlier removal with IQR\n",
    "def drop_outliers_iqr(df):\n",
    "    q1 = df.quantile(0.25)\n",
    "    q3 = df.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "    mask = ((df < lower) | (df > upper)).any(axis=1)\n",
    "    return df[~mask]\n",
    "\n",
    "cleaned_df = drop_outliers_iqr(data)\n",
    "\n",
    "# Normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "cleaned_df_normalized = scaler.fit_transform(cleaned_df)\n",
    "cleaned_df_normalized_df = pd.DataFrame(cleaned_df_normalized, columns=cleaned_df.columns)\n",
    "\n",
    "x = cleaned_df_normalized_df.drop(columns=['Patv'])\n",
    "y = cleaned_df_normalized_df['Patv'].to_numpy()  # make numpy array for later\n",
    "\n",
    "# ===========================\n",
    "# Manual TOPSIS Feature Ranking (feature-level)\n",
    "# ===========================\n",
    "# We treat each feature (column) as an alternative by transposing the matrix.\n",
    "decision_matrix = x.values.T.astype(float)   # shape: (n_features, n_samples)\n",
    "eps = 1e-12  # small value to avoid divide-by-zero\n",
    "\n",
    "# Normalize across samples for each feature (vector normalization)\n",
    "denom = np.sqrt((decision_matrix ** 2).sum(axis=1, keepdims=True))\n",
    "denom[denom == 0] = eps\n",
    "norm_matrix = decision_matrix / denom  # shape: (n_features, n_samples)\n",
    "\n",
    "# Use equal weights for criteria (samples) when treating features as alternatives\n",
    "# (This is a heuristic; you can change weights if desired)\n",
    "weights = np.ones(norm_matrix.shape[1])  # one weight per sample/criterion\n",
    "weighted_matrix = norm_matrix * weights[np.newaxis, :]\n",
    "\n",
    "# Ideal best and ideal worst (for each criterion/sample)\n",
    "ideal_best = weighted_matrix.max(axis=0)\n",
    "ideal_worst = weighted_matrix.min(axis=0)\n",
    "\n",
    "# Distances of each feature (alternative) to ideal best/worst\n",
    "dist_best = np.sqrt(((weighted_matrix - ideal_best[np.newaxis, :]) ** 2).sum(axis=1))\n",
    "dist_worst = np.sqrt(((weighted_matrix - ideal_worst[np.newaxis, :]) ** 2).sum(axis=1))\n",
    "\n",
    "# Closeness coefficient per feature\n",
    "closeness = dist_worst / (dist_best + dist_worst + eps)\n",
    "\n",
    "feature_scores = pd.DataFrame({\n",
    "    \"Feature\": x.columns,\n",
    "    \"Closeness\": closeness\n",
    "})\n",
    "feature_scores[\"Rank\"] = feature_scores[\"Closeness\"].rank(ascending=False, method='min')\n",
    "feature_scores = feature_scores.sort_values(by=\"Rank\").reset_index(drop=True)\n",
    "\n",
    "print(\"\\nTOPSIS Feature Ranking:\")\n",
    "print(feature_scores)\n",
    "\n",
    "# Select top 6 features\n",
    "top_features = feature_scores.head(6)[\"Feature\"].tolist()\n",
    "df_reduced = x[top_features].copy()\n",
    "print(\"\\nSelected Features via TOPSIS:\")\n",
    "print(df_reduced.head())\n",
    "\n",
    "# ===========================\n",
    "# Train/Test Split\n",
    "# ===========================\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_reduced, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Convert y to numpy arrays (if not already)\n",
    "y_train = np.asarray(y_train).reshape(-1)\n",
    "y_test = np.asarray(y_test).reshape(-1)\n",
    "\n",
    "# ===========================\n",
    "# VAE Model (fixed shapes + KL via add_loss)\n",
    "# ===========================\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "latent_dim = 6\n",
    "n_features = x_train.shape[1]  # should be 6\n",
    "\n",
    "# VAE input shape: (timesteps/features, channels=1)\n",
    "encoder_inputs = layers.Input(shape=(n_features, 1), name=\"encoder_input\")\n",
    "\n",
    "# Encoder\n",
    "e = layers.Conv1D(64, kernel_size=3, activation='elu', padding='same')(encoder_inputs)\n",
    "e = layers.Conv1D(32, kernel_size=3, activation='elu', padding='same')(e)\n",
    "e = layers.Flatten()(e)\n",
    "e = layers.Dense(128, activation='relu')(e)\n",
    "\n",
    "z_mean = layers.Dense(latent_dim, name='z_mean')(e)\n",
    "z_log_var = layers.Dense(latent_dim, name='z_log_var')(e)\n",
    "\n",
    "def sampling_layer(args):\n",
    "    zm, zv = args\n",
    "    eps = tf.random.normal(shape=tf.shape(zm))\n",
    "    return zm + tf.exp(0.5 * zv) * eps\n",
    "\n",
    "z = layers.Lambda(sampling_layer, name='z')([z_mean, z_log_var])\n",
    "\n",
    "encoder = models.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "# Decoder (produce same shape as input: (n_features, 1))\n",
    "decoder_inputs = layers.Input(shape=(latent_dim,), name=\"decoder_input\")\n",
    "d = layers.Dense(128, activation='relu')(decoder_inputs)\n",
    "d = layers.Dense(n_features * 16, activation='relu')(d)\n",
    "d = layers.Reshape((n_features, 16))(d)\n",
    "d = layers.Conv1D(16, kernel_size=3, activation='elu', padding='same')(d)\n",
    "decoder_outputs = layers.Conv1D(1, kernel_size=3, activation='sigmoid', padding='same', name='decoder_output')(d)\n",
    "\n",
    "decoder = models.Model(decoder_inputs, decoder_outputs, name='decoder')\n",
    "\n",
    "# VAE model: encoder -> decoder(z)\n",
    "encoded = encoder(encoder_inputs)[2]\n",
    "reconstructed = decoder(encoded)\n",
    "vae = models.Model(encoder_inputs, reconstructed, name='vae')\n",
    "\n",
    "# Add KL loss via add_loss (works with Keras symbolic tensors)\n",
    "kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "vae.add_loss(kl_loss)\n",
    "\n",
    "# Compile with reconstruction loss only (KL is added already)\n",
    "vae.compile(optimizer=Adam(1e-3), loss='mse')\n",
    "\n",
    "# Prepare inputs: expand dims to add channel dimension\n",
    "X_train_vae = np.expand_dims(x_train.values.astype(np.float32), axis=-1)  # shape (samples, n_features, 1)\n",
    "X_test_vae = np.expand_dims(x_test.values.astype(np.float32), axis=-1)\n",
    "\n",
    "# Fit VAE\n",
    "vae.fit(X_train_vae, X_train_vae,\n",
    "        validation_data=(X_test_vae, X_test_vae),\n",
    "        epochs=50, batch_size=64)\n",
    "\n",
    "# ===========================\n",
    "# Get latent representations\n",
    "# ===========================\n",
    "latent_train = encoder.predict(X_train_vae)[2]  # shape (n_samples, latent_dim)\n",
    "latent_test = encoder.predict(X_test_vae)[2]\n",
    "\n",
    "# reshape for LSTM input: (samples, timesteps=latent_dim, features=1)\n",
    "latent_train_reshaped = latent_train.reshape(latent_train.shape[0], latent_train.shape[1], 1)\n",
    "latent_test_reshaped = latent_test.reshape(latent_test.shape[0], latent_test.shape[1], 1)\n",
    "\n",
    "# ===========================\n",
    "# LSTM on Latent Features with KFold\n",
    "# ===========================\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "latent_data = np.concatenate((latent_train_reshaped, latent_test_reshaped), axis=0)\n",
    "target_data = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold = 1\n",
    "for train_idx, test_idx in kf.split(latent_data):\n",
    "    print(f\"\\nTraining on Fold {fold}...\")\n",
    "    X_tr, X_val = latent_data[train_idx], latent_data[test_idx]\n",
    "    Y_tr, Y_val = target_data[train_idx], target_data[test_idx]\n",
    "\n",
    "    model = Sequential([\n",
    "        LSTM(128, activation='tanh', return_sequences=True, input_shape=(latent_dim, 1)),\n",
    "        Dropout(0.2),\n",
    "        LSTM(64, activation='tanh'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(1e-4), loss='mse')\n",
    "    model.fit(X_tr, Y_tr, epochs=10, batch_size=16, verbose=1)\n",
    "\n",
    "    loss = model.evaluate(X_val, Y_val, verbose=0)\n",
    "    print(f\"Fold {fold} - Test Loss: {loss}\")\n",
    "\n",
    "    Y_pred = model.predict(X_val)\n",
    "    print(f\"Fold {fold} - MSE: {mean_squared_error(Y_val, Y_pred)}\")\n",
    "    print(f\"Fold {fold} - MAE: {mean_absolute_error(Y_val, Y_pred)}\")\n",
    "    print(f\"Fold {fold} - R2: {r2_score(Y_val, Y_pred)}\")\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# Save final model (last fold model)\n",
    "model.save('vae_lstm_update.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce91b391-fbe5-4799-a8e9-eb31fe00d592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
